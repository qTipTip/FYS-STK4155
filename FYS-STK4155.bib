
@online{hoerlRidgeRegressionBiased,
  title = {Ridge {{Regression}}: {{Biased Estimation}} for {{Nonorthogonal Problems}}: {{Technometrics}}: {{Vol}} 12, {{No}} 1},
  url = {https://www.tandfonline.com/doi/abs/10.1080/00401706.1970.10488634},
  urldate = {2019-09-24},
  author = {Hoerl, Arthur E. and Kennard, Robert W.},
  file = {/home/ivar/Zotero/storage/UALF6SL5/00401706.1970.html}
}

@book{tikhonovNumericalMethodsSolution1995,
  langid = {english},
  title = {Numerical {{Methods}} for the {{Solution}} of {{Ill}}-{{Posed Problems}}},
  isbn = {978-0-7923-3583-2},
  url = {https://www.springer.com/us/book/9780792335832},
  abstract = {Many problems in science, technology and engineering are posed in the form of operator equations of the first kind, with the operator and RHS approximately known. But such problems often turn out to be ill-posed, having no solution, or a non-unique solution, and/or an unstable solution. Non-existence and non-uniqueness can usually be overcome by settling for `generalised' solutions, leading to the need to develop regularising algorithms. The theory of ill-posed problems has advanced greatly since A. N. Tikhonov laid its foundations, the Russian original of this book (1990) rapidly becoming a classical monograph on the topic. The present edition has been completely updated to consider linear ill-posed problems with or without a priori constraints (non-negativity, monotonicity, convexity, etc.). Besides the theoretical material, the book also contains a FORTRAN program library. Audience: Postgraduate students of physics, mathematics, chemistry, economics, engineering. Engineers and scientists interested in data processing and the theory of ill-posed problems.},
  series = {Mathematics and {{Its Applications}}},
  publisher = {{Springer Netherlands}},
  urldate = {2019-09-24},
  date = {1995},
  author = {Tikhonov, A. N. and Goncharsky, A. and Stepanov, V. V. and Yagola, Anatoly G.},
  file = {/home/ivar/Zotero/storage/SPGDS3H2/9780792335832.html}
}

@article{skyttLocallyRefinedSpline2015,
  title = {Locally Refined Spline Surfaces for Representation of Terrain Data},
  volume = {49},
  issn = {0097-8493},
  url = {http://www.sciencedirect.com/science/article/pii/S0097849315000308},
  doi = {10.1016/j.cag.2015.03.006},
  abstract = {In this paper we describe the use of a novel representation, LR B-spline surfaces, and apply this representation in the treatment of geographical data. These data sets are typically very large and LR B-spline surfaces offer a compact representation of overall smooth data with local details. We briefly describe the properties of the LR B-spline representation, and discuss the details of two approximation methods adapted for LR B-splines: least squares approximation and multilevel B-spline approximation (MBA). The described techniques are demonstrated on several examples of terrain data in the form of point clouds.},
  journaltitle = {Computers \& Graphics},
  shortjournal = {Computers \& Graphics},
  urldate = {2019-09-24},
  date = {2015-06-01},
  pages = {58-68},
  keywords = {Approximation methods,LR B-spline surface,Point clouds,Terrain data},
  author = {Skytt, Vibeke and Barrowclough, Oliver and Dokken, Tor},
  file = {/home/ivar/Zotero/storage/3JPHM76U/Skytt et al. - 2015 - Locally refined spline surfaces for representation.pdf;/home/ivar/Zotero/storage/3V67JRUF/S0097849315000308.html}
}

@article{mehtaHighbiasLowvarianceIntroduction2019,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1803.08823},
  title = {A High-Bias, Low-Variance Introduction to {{Machine Learning}} for Physicists},
  volume = {810},
  issn = {03701573},
  url = {http://arxiv.org/abs/1803.08823},
  doi = {10.1016/j.physrep.2019.03.001},
  abstract = {Machine Learning (ML) is one of the most exciting and dynamic areas of modern research and application. The purpose of this review is to provide an introduction to the core concepts and tools of machine learning in a manner easily understood and intuitive to physicists. The review begins by covering fundamental concepts in ML and modern statistics such as the bias-variance tradeoff, overfitting, regularization, generalization, and gradient descent before moving on to more advanced topics in both supervised and unsupervised learning. Topics covered in the review include ensemble models, deep learning and neural networks, clustering and data visualization, energy-based models (including MaxEnt models and Restricted Boltzmann Machines), and variational methods. Throughout, we emphasize the many natural connections between ML and statistical physics. A notable aspect of the review is the use of Python Jupyter notebooks to introduce modern ML/statistical packages to readers using physics-inspired datasets (the Ising Model and Monte-Carlo simulations of supersymmetric decays of proton-proton collisions). We conclude with an extended outlook discussing possible uses of machine learning for furthering our understanding of the physical world as well as open problems in ML where physicists may be able to contribute. (Notebooks are available at https://physics.bu.edu/\textasciitilde{}pankajm/MLnotebooks.html )},
  journaltitle = {Physics Reports},
  shortjournal = {Physics Reports},
  urldate = {2019-09-12},
  date = {2019-05},
  pages = {1-124},
  keywords = {Computer Science - Machine Learning,Condensed Matter - Statistical Mechanics,Physics - Computational Physics,Statistics - Machine Learning},
  author = {Mehta, Pankaj and Bukov, Marin and Wang, Ching-Hao and Day, Alexandre G. R. and Richardson, Clint and Fisher, Charles K. and Schwab, David J.},
  file = {/home/ivar/Zotero/storage/YXZMDT3H/Mehta et al. - 2019 - A high-bias, low-variance introduction to Machine .pdf;/home/ivar/Zotero/storage/28QZM45L/1803.html}
}

@article{vanwieringenLectureNotesRidge2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1509.09169},
  primaryClass = {stat},
  title = {Lecture Notes on Ridge Regression},
  url = {http://arxiv.org/abs/1509.09169},
  abstract = {The linear regression model cannot be fitted to high-dimensional data, as the high-dimensionality brings about empirical non-identifiability. Penalized regression overcomes this non-identifiability by augmentation of the loss function by a penalty (i.e. a function of regression coefficients). The ridge penalty is the sum of squared regression coefficients, giving rise to ridge regression. Here many aspect of ridge regression are reviewed e.g. moments, mean squared error, its equivalence to constrained estimation, and its relation to Bayesian regression. Finally, its behaviour and use are illustrated in simulation and on omics data. Subsequently, ridge regression is generalized to allow for a more general penalty. The ridge penalization framework is then translated to logistic regression and its properties are shown to carry over. To contrast ridge penalized estimation, the final chapter introduces its lasso counterpart.},
  urldate = {2019-09-12},
  date = {2015-09-30},
  keywords = {Statistics - Methodology},
  author = {van Wieringen, Wessel N.},
  options = {useprefix=true},
  file = {/home/ivar/Zotero/storage/97EDUDG4/van Wieringen - 2015 - Lecture notes on ridge regression.pdf;/home/ivar/Zotero/storage/2GAJGNY2/1509.html}
}

@online{10PDFLecture,
  langid = {english},
  title = {(10) ({{PDF}}) {{Lecture}} Notes on Ridge Regression},
  url = {https://www.researchgate.net/publication/282403184_Lecture_notes_on_ridge_regression},
  abstract = {ResearchGate is a network dedicated to science and research. Connect, collaborate and discover scientific publications, jobs and conferences. All for free.},
  journaltitle = {ResearchGate},
  urldate = {2019-09-12},
  file = {/home/ivar/Zotero/storage/TJRYBJQ9/282403184_Lecture_notes_on_ridge_regression.html}
}

@report{frankeCriticalComparisonMethods1979,
  langid = {english},
  title = {A {{Critical Comparison}} of {{Some Methods}} for {{Interpolation}} of {{Scattered Data}}},
  url = {https://apps.dtic.mil/docs/citations/ADA081688},
  abstract = {This report is concerned with methods for solving the scattered data interpolation problem: Given points (X sub K, Y sub k, F sub k), k = 1, ..., N, construct a smooth function, F(x,y), so that F(X sub k, Y sub k) = F sub k, K = 1, ...,N. A comparison of 29 methods for solution of this problem has been made. Each of the methods is discussed and the results of extensive testing for their properties and appropriate values of their parameters is given. Both local and global methods are considered. Comparisons of timing, storage, accuracy, visual pleasantness of the surface, and ease of implementation are made. A large number (over 200) of pages of perspective plots of surfaces are given. Suggestions for improvement of some methods are made, and methods which have poor approximation properties are identified.},
  number = {NPS53-79-003},
  institution = {{NAVAL POSTGRADUATE SCHOOL MONTEREY CA}},
  urldate = {2019-08-19},
  date = {1979-12},
  author = {Franke, Richard},
  file = {/home/ivar/Zotero/storage/GC9GZ3C7/Franke - 1979 - A Critical Comparison of Some Methods for Interpol.pdf;/home/ivar/Zotero/storage/KUU85T3E/ADA081688.html}
}

@software{SkorchdevSkorch2019,
  title = {Skorch-Dev/Skorch},
  url = {https://github.com/skorch-dev/skorch},
  abstract = {A scikit-learn compatible neural network library that wraps pytorch},
  organization = {{skorch-dev}},
  urldate = {2019-12-13},
  date = {2019-12-13T01:08:15Z},
  keywords = {machine-learning,pytorch,scikit-learn},
  origdate = {2017-07-18T00:13:54Z}
}

@article{mchughInterraterReliabilityKappa2012,
  title = {Interrater Reliability: The Kappa Statistic},
  volume = {22},
  issn = {1330-0962},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3900052/},
  shorttitle = {Interrater Reliability},
  abstract = {The kappa statistic is frequently used to test interrater reliability. The importance of rater reliability lies in the fact that it represents the extent to which the data collected in the study are correct representations of the variables measured. Measurement of the extent to which data collectors (raters) assign the same score to the same variable is called interrater reliability. While there have been a variety of methods to measure interrater reliability, traditionally it was measured as percent agreement, calculated as the number of agreement scores divided by the total number of scores. In 1960, Jacob Cohen critiqued use of percent agreement due to its inability to account for chance agreement. He introduced the Cohen’s kappa, developed to account for the possibility that raters actually guess on at least some variables due to uncertainty. Like most correlation statistics, the kappa can range from −1 to +1. While the kappa is one of the most commonly used statistics to test interrater reliability, it has limitations. Judgments about what level of kappa should be acceptable for health research are questioned. Cohen’s suggested interpretation may be too lenient for health related studies because it implies that a score as low as 0.41 might be acceptable. Kappa and percent agreement are compared, and levels for both kappa and percent agreement that should be demanded in healthcare studies are suggested.},
  number = {3},
  journaltitle = {Biochemia Medica},
  shortjournal = {Biochem Med (Zagreb)},
  urldate = {2019-12-16},
  date = {2012-10-15},
  pages = {276-282},
  author = {McHugh, Mary L.},
  eprinttype = {pmid},
  eprint = {23092060},
  pmcid = {PMC3900052}
}

@online{PyTorch,
  langid = {english},
  title = {{{PyTorch}}},
  url = {https://www.pytorch.org},
  abstract = {An open source deep learning platform that provides a seamless path from research prototyping to production deployment.},
  urldate = {2019-12-16},
  file = {/home/ivar/Zotero/storage/UM6LD254/pytorch.org.html}
}

@software{stangebyFYSSTK4155Project2019,
  title = {{{FYS}}-{{STK4155}} - {{Project}} 1},
  url = {https://github.com/qTipTip/FYS-STK4155},
  abstract = {Coursework for FYS-STK4155. Contribute to qTipTip/FYS-STK4155 development by creating an account on GitHub.},
  urldate = {2019-12-16},
  date = {2019-12-12T14:54:29Z},
  author = {Stangeby, Ivar},
  origdate = {2019-09-09T09:11:54Z}
}

@article{hancockLinearPartialDifferential,
  langid = {english},
  title = {Linear {{Partial Differential Equations}}},
  pages = {44},
  author = {Hancock, Matthew J},
  file = {/home/ivar/Zotero/storage/ZK4XBLU8/Linear Partial Differential Equations.pdf}
}

@article{luDeepXDEDeepLearning2019,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1907.04502},
  primaryClass = {physics, stat},
  title = {{{DeepXDE}}: {{A}} Deep Learning Library for Solving Differential Equations},
  url = {http://arxiv.org/abs/1907.04502},
  shorttitle = {{{DeepXDE}}},
  abstract = {Deep learning has achieved remarkable success in diverse applications; however, its use in solving partial differential equations (PDEs) has emerged only recently. Here, we present an overview of physics-informed neural networks (PINNs), which embed a PDE into the loss of the neural network using automatic differentiation. The PINN algorithm is simple, and it can be applied to different types of PDEs, including integro-differential equations, fractional PDEs, and stochastic PDEs. Moreover, PINNs solve inverse problems as easily as forward problems. We propose a new residual-based adaptive refinement (RAR) method to improve the training efficiency of PINNs. For pedagogical reasons, we compare the PINN algorithm to a standard finite element method. We also present a Python library for PINNs, DeepXDE, which is designed to serve both as an education tool to be used in the classroom as well as a research tool for solving problems in computational science and engineering. DeepXDE supports complex-geometry domains based on the technique of constructive solid geometry, and enables the user code to be compact, resembling closely the mathematical formulation. We introduce the usage of DeepXDE and its customizability, and we also demonstrate the capability of PINNs and the user-friendliness of DeepXDE for five different examples. More broadly, DeepXDE contributes to the more rapid development of the emerging Scientific Machine Learning field.},
  urldate = {2019-12-18},
  date = {2019-07-10},
  keywords = {Computer Science - Machine Learning,Physics - Computational Physics,Statistics - Machine Learning},
  author = {Lu, Lu and Meng, Xuhui and Mao, Zhiping and Karniadakis, George E.},
  file = {/home/ivar/Zotero/storage/UXMQ4FGX/Lu et al. - 2019 - DeepXDE A deep learning library for solving diffe.pdf;/home/ivar/Zotero/storage/8TM4S296/1907.html}
}

@article{al-aradiSolvingNonlinearHighDimensional,
  langid = {english},
  title = {Solving {{Nonlinear}} and {{High}}-{{Dimensional Partial Differential Equations}} via {{Deep Learning}}},
  pages = {76},
  author = {Al-Aradi, Ali and Correia, Adolfo and Naiff, Danilo and Jardim, Gabriel and Vargas, Fundacao Getulio and Saporito, Yuri},
  file = {/home/ivar/Zotero/storage/Y2W5EJI6/Al-Aradi et al. - Solving Nonlinear and High-Dimensional Partial Dif.pdf}
}

@software{stangebyFYSSTK4155Project2019a,
  title = {{{FYS}}-{{STK4155}} - {{Project}} 2},
  url = {https://github.com/qTipTip/FYS-STK4155},
  abstract = {Coursework for FYS-STK4155. Contribute to qTipTip/FYS-STK4155 development by creating an account on GitHub.},
  urldate = {2019-12-18},
  date = {2019-12-17T22:51:13Z},
  author = {Stangeby, Ivar},
  origdate = {2019-09-09T09:11:54Z}
}



\relax 
\providecommand{\transparent@use}[1]{}
\providecommand\hyper@newdestlabel[2]{}
\providecommand*{\memsetcounter}[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {chapter}{\chapternumberline {1}Introduction}{1}{chapter.1}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\chapternumberline {2}Background}{1}{chapter.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Neural Networks}{1}{section.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Construction}{1}{section*.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces A simple feed forward neural network. This specific network has an input layer of four nodes. Two hidden layers consisting of six nodes each, and finally, an output layer of size two.}}{2}{figure.2.1}\protected@file@percent }
\newlabel{fig:fc_nn}{{1}{2}{A simple feed forward neural network. This specific network has an input layer of four nodes. Two hidden layers consisting of six nodes each, and finally, an output layer of size two}{figure.2.1}{}}
\newlabel{fig:fc_nn@cref}{{[figure][1][]1}{[1][1][]2}}
\@writefile{toc}{\contentsline {subsection}{Optimization}{2}{section*.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Neural Network Architecture}{3}{section*.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Credit Card Data}{3}{section*.4}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Regression Data}{3}{section*.5}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\chapternumberline {3}Implementation}{3}{chapter.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Data Wrangling}{3}{section*.6}\protected@file@percent }
\@writefile{lol}{\contentsline {listing}{\numberline {1}{\ignorespaces Interfacing the data-sets with a PyTorch API, which facilitates easy iteration as shown in \cref  {lst:training_loop}}}{4}{listing.1}\protected@file@percent }
\newlabel{lst:datasets}{{1}{4}{Neural Network}{listing.1}{}}
\newlabel{lst:datasets@cref}{{[listing][1][]1}{[1][4][]4}}
\@writefile{lol}{\contentsline {listing}{\numberline {2}{\ignorespaces A simple training loop using PyTorch data-loaders.}}{4}{listing.2}\protected@file@percent }
\newlabel{lst:training_loop}{{2}{4}{Neural Network}{listing.2}{}}
\newlabel{lst:training_loop@cref}{{[listing][2][]2}{[1][4][]4}}
\@writefile{toc}{\contentsline {subsection}{Neural Network}{4}{section*.7}\protected@file@percent }
\@writefile{lol}{\contentsline {listing}{\numberline {3}{\ignorespaces A generic dense neural network realized in the PyTorch framework. By implementing the forward method, gradients are kept track of automatically, and hence no consideration is to be made for the backward propagation.}}{5}{listing.3}\protected@file@percent }
\newlabel{lst:neural_network}{{3}{5}{Neural Network}{listing.3}{}}
\newlabel{lst:neural_network@cref}{{[listing][3][]3}{[1][4][]5}}
\@writefile{toc}{\contentsline {subsection}{Classical Methods}{5}{section*.8}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Numerical Results}{5}{section.3.1}\protected@file@percent }
\gdef\minted@oldcachelist{,
  default-pyg-prefix.pygstyle,
  algol.pygstyle,
  3A21AD3B3FA404A6522AC228E957EB0A4FBEEAFC55D0059826E664B9BBCF3721.pygtex,
  87A9284AB7F4FFD77E8996736377DC0D4FBEEAFC55D0059826E664B9BBCF3721.pygtex,
  508BA40AC3B87B48DD09D6DEEF82EED64FBEEAFC55D0059826E664B9BBCF3721.pygtex,
  0C3DFED29FF869823AA51B29A68598F44FBEEAFC55D0059826E664B9BBCF3721.pygtex,
  229C0792C185F88847AB4E4B1BD617944FBEEAFC55D0059826E664B9BBCF3721.pygtex,
  755A200471CE391F7EBCA0A12A0793904FBEEAFC55D0059826E664B9BBCF3721.pygtex}
\@writefile{lol}{\contentsline {listing}{\numberline {4}{\ignorespaces Setting up and training the logistic regressor. We use both \(L^1\) and \( L^2\)-regularization, corresponding to Lasso and Ridge-regression, respectively. We find the optimal regularization-strength by 5-fold cross validation over the data-set. The fits are parallelized over all available CPU-cores, to speed up the process.}}{6}{listing.4}\protected@file@percent }
\newlabel{lst:classifier_credit_card}{{4}{6}{Classical Methods}{listing.4}{}}
\newlabel{lst:classifier_credit_card@cref}{{[listing][4][]4}{[1][4][]6}}
\@writefile{toc}{\contentsline {subsection}{Credit card data}{6}{section*.9}\protected@file@percent }
\@writefile{toc}{\contentsline {appendix}{\chapternumberline {A}Regularized neural network}{6}{appendix.A}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Franke function}{6}{section*.10}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces The results of the \( L^1 \) and \( L^2 \) regularized logistic regression. The two regularization types seems to converge to exactly the same classification accuracy as non-penalized logistic regression, with this value being 81.6\%. As \( \lambda \) increases, the penalization term is set to zero, hence converges to non-penalized regression. Why this coincides with the best fit is not clear to me. }}{7}{figure.3.2}\protected@file@percent }
\newlabel{fig:credit_cards_classical}{{2}{7}{The results of the \( L^1 \) and \( L^2 \) regularized logistic regression. The two regularization types seems to converge to exactly the same classification accuracy as non-penalized logistic regression, with this value being 81.6\%. As \( \lambda \) increases, the penalization term is set to zero, hence converges to non-penalized regression. Why this coincides with the best fit is not clear to me}{figure.3.2}{}}
\newlabel{fig:credit_cards_classical@cref}{{[figure][2][]2}{[1][6][]7}}
\@writefile{lol}{\contentsline {listing}{\numberline {5}{\ignorespaces By wrapping the GenericNN in a RegularizedNeuralNetClassifier (see \cref  {lst:rnnc}) we may train the PyTorch model using the GridSearchCV-functionality from scikit-learn.}}{8}{listing.5}\protected@file@percent }
\newlabel{lst:classification_nn}{{5}{8}{Credit card data}{listing.5}{}}
\newlabel{lst:classification_nn@cref}{{[listing][5][]5}{[1][6][]8}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces The classification accuracy as a function of the learning rate for the generic neural network. At the opimal learning rate, the network achieves a prediction accuracy of approximately 82.54\%, which is not insignificantly better than all the classical methods. The filled blue regions represent the standard deviations as reported by \texttt  {GridSearchCV}. Note how the network at performs it's worst by guessing only zeros, similarily to the overpenalized logistic regression. The network is trained over 20 epochs, using SGD with momentum and 5-fold cross validation. }}{9}{figure.3.3}\protected@file@percent }
\newlabel{fig:credit_card_nn}{{3}{9}{The classification accuracy as a function of the learning rate for the generic neural network. At the opimal learning rate, the network achieves a prediction accuracy of approximately 82.54\%, which is not insignificantly better than all the classical methods. The filled blue regions represent the standard deviations as reported by \texttt {GridSearchCV}. Note how the network at performs it's worst by guessing only zeros, similarily to the overpenalized logistic regression. The network is trained over 20 epochs, using SGD with momentum and 5-fold cross validation}{figure.3.3}{}}
\newlabel{fig:credit_card_nn@cref}{{[figure][3][]3}{[1][6][]9}}
\@writefile{lol}{\contentsline {listing}{\numberline {6}{\ignorespaces We implemented a regularized neural network classifier in order to be able to compare the network with both \( L^2 \) and \( L^1 \) regularization applied to the network weights. Unfortunately, we were not able to perform the required numerical tests in time, as the parameter space is rather large.}}{10}{listing.6}\protected@file@percent }
\newlabel{lst:rnnc}{{6}{10}{Regularized neural network}{listing.6}{}}
\newlabel{lst:rnnc@cref}{{[listing][6][2147483647]6}{[1][6][]10}}
\memsetcounter{lastsheet}{10}
\memsetcounter{lastpage}{10}

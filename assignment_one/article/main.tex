\documentclass[dvipsnames, article, a4paper, oneside, 12pt]{memoir}
\linespread{1.25}
%\setlength{\parskip}{1em}
%\setlength{\parindent}{0em}

\usepackage[]{mathtools}
\usepackage[]{amssymb}
\usepackage[]{libertine}
\usepackage[]{libertinust1math}
\usepackage[T1]{fontenc}
\usepackage[]{microtype}
\usepackage[]{geometry}
\usepackage[]{amsthm}
\usepackage[]{thmtools}
\usepackage[usedvipsnames]{xcolor}
\usepackage[]{bm}
\usepackage[]{amsmath}
\usepackage[]{commath}
\usepackage[]{todonotes}
\usepackage[noabbrev, capitalize]{cleveref}
\usepackage[]{graphicx}
\usepackage{titling}

\declaretheorem[style=remark, name=Remark,
mdframed={
}]{remark}
\declaretheorem[style=remark, name=Assumption,
mdframed={
}]{assumption}

\title{\textsc{Regression And Resampling Techniques \\
  FYS-STK4155 \\
Assignment One}}
\author{Ivar Haugal{\o}kken Stangeby}

\newcommand{\x}{\bm{x}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\X}{\bm{X}}
\newcommand{\diag}[1]{\mathrm{diag}(#1)}
\newcommand{\MSE}[1]{\mathrm{MSE}(#1)}
\newcommand{\R}[1]{\mathrm{R}^2(#1)}
\newcommand{\expect}{\mathbb{E}}
\newcommand{\OLS}{\mathrm{OLS}}
\newcommand{\ridge}{\mathrm{ridge}}

\newcommand{\mat}[1]{\bm{#1}}
\newcommand{\y}{\bm{y}}
\newcommand{\data}{\bm{\Omega}}
\newcommand{\cost}{\mathcal{C}}
\newcommand{\N}{\mathcal{N}}
\newcommand{\basis}{\mathcal{B}}
%\newcommand{\set}[1]{\left\{\, #1 \right\}}

\pretitle{%
  \begin{center}
    \includegraphics[scale=0.7]{images/franke_shadow.pdf}\\[\bigskipamount]
}
\posttitle{\end{center}}

\begin{document}
  \maketitle 
  
  \tableofcontents

  \chapter{Introduction}
   
  Regression analysis is the act of estimating relationships among variables.
  In this project, we study various regression methods in more detail. In
  particular, we compare the \emph{ordinary least squares} (OLS) method with
  the \emph{Ridge regression} and \emph{Lasso regression} techniques. As we
  shall see, these three methods are all variations over the same theme. We
  start by testing the methods on noisy data sampled from a function known as
  \emph{Franke's} bivariate test function (visualized on the title page), which
  has been in widespread use in the evaluation of interpolation and data
  fitting techniques. Finally, we run regression on real terrain data,
  comparing the aforementioned methods.

  \chapter{Regression Techniques}

  In general, the goal of regression analysis is to fit a \emph{model function}
  \( f(\x, \beta) \) to a set of \( n \) data points \( \data = (\x_i,
  y_i)_{i=1}^n \). A simple example is that of a linear polynomial with two
  parameters:
  \begin{equation}
    f(x, \beta) = \beta_0 + \beta_1 x.
  \end{equation}
  The \emph{model parameters} \( \beta \) are determined in order to minimize a
  suitable \emph{cost function} \(\cost(\data, \beta)\) which measures to which
  extent the model function manages to capture trends in the data \( \data \).
  It is the choice of cost function \( \mathcal{C}(\data, \beta)\) which
  distinguishes  the three regression techniques, OLS, Lasso regression, and
  Ridge regression.

  \begin{assumption}
    \label{as:general_linear}
    It is often assumed a priori that the data is infact generated from a noisy
    model, such that each \( y_i \) can be described as
    \begin{equation}
      y_i = f(\x_i) + \varepsilon_i
    \end{equation}
    where each \( \varepsilon_i \sim \N(0, \sigma^2) \) is normally distributed
    with zero mean and variance \( \sigma^2 \). This assumption on the error
    gives rise to what is known as \emph{general linear models}.
  \end{assumption}


  \paragraph{The design matrix.} We are often interested in finding the best
  model function in a specific function space.  Assuming this space has a basis
  \( \basis = \set{\varphi_i}_{i=1}^M\), we may write our model function in
  terms of the basis functions and the model parameters as
  \begin{equation}
    \label{eq:gen_lin_mod}
    f(\x, \beta) \coloneqq \sum_{i = 1}^M \beta_i\varphi_i(\x).
  \end{equation}
  As an example, if we were to use the space \( \Pi_2^2 \) of bi-quadratic
  polynomials, our basis functions would be
  \begin{equation}
    \basis = \set{1, x, x^2, y, y^2, xy}.
  \end{equation}
  With this formulation, we can represent the approximation \( \hat{\y} \) to
  \( \y \) as a matrix product 
  \begin{equation}
    \label{eq:matrix_prod}
    \hat{\y} = \X\beta
  \end{equation}
  where \( \X \) is the \emph{design matrix} defined by components \(\X_{ij} =
  \varphi_j(\x_i)\), and \( \beta = [\beta_1, \ldots, \beta_M] \) is the model
  parameters. 

  \paragraph{Performance metrics.}
  In order to evaluate the how accurately the model function \( f(\data, \beta)
  \) captures trends in the target data \( \y \), a few standard performance
  metrics are  used.  Firstly, the \emph{mean squared error}:
  \begin{equation}
    \MSE{\y, \hat{\y}} \coloneqq \frac{1}{n} \sum_{i=1}^n (y_i - \hat{y}_i)^2
  \end{equation}
  which simply averages the squared error over all samples and estimates.
  Secondly, we have the \emph{coefficient of determination} or
  \emph{\(R^2\)-score}:
  \begin{equation}
    \R{\y, \hat{y}} \coloneqq 1 - \frac{\sum_{i=1}^n (y_i -
    \hat{y}_i)^2}{\sum_{i=1}^n (y_i - \overline{y})^2}.
  \end{equation}
  The \(R^2\)-score measures much of the variation in \( \y \) which can be
  attributed to a simple linear relation between \( \x \) and \( \y \). It is a
  ratio, thus a value of one tells us that all the variation in the data can be
  attributed to an approximate linear relationship between the data \(\x\) and
  the response variable \( \y \). A value of zero indicates that a non-linear
  model may be preferrable.
  

  \section{Ordinary Least Squares}
  
  One common cost function is the one which involves the sum of squared
  residuals (or squared errors):
  \begin{equation}
    \label{eq:mse}
    \cost(\data, \beta) = \frac{1}{n}\sum_{i = 1}^n \varepsilon_i^2 \coloneqq \frac{1}{n}\sum_{i=1}^n (y_i - \hat{y}_i)^2
  \end{equation}
  where \( \hat{y}_i \coloneqq f(\x_i, \beta) \). The method involving the
  minimization of this specific cost function is known as \emph{least squares}.
  With the matrix notation from above in mind, we can also write the cost
  function as
  \begin{equation}
    \cost(\data, \beta) = \frac{1}{n} (\y - \hat{\y})^T (\y - \hat{\y}).
  \end{equation}

  

  \subsection{Optimizing the parameters in OLS.}
  
  Assume now that our model is of the form given in \cref{eq:gen_lin_mod} and
  that our cost function is the mean squared error defined in \cref{eq:mse}. We
  are interested in finding the parameters \( \beta \) that minimize the cost
  function \( \cost(\data, \beta) \). Since \( \cost(\data, \beta) \) is
  convex, it suffices to differentiate with respect to \( \beta \) and equating
  to zero. 
  
  We have that
  \begin{align}
    \dpd{\cost(\data, \beta)}{\beta} = \X^T(\y - \X\beta), 
  \end{align}
  and equating this to zero yields the familiar \emph{normal equations}
  \begin{equation}
    \X^T \y = \X^T\X\beta.
  \end{equation}
  If the matrix \( \X^T\X \) is invertible, we may obtain the solution by
  direct numerical matrix inversion. In this case, the optimal model parameters
  are found directly by
  \begin{equation}
    \label{eq:optimal}
    \beta_{\OLS} = (\X^T\X)^{-1}\X^T\y.
  \end{equation}
  However, these matrices may be ill-conditioned when the number of equations
  are very large, and it is therefore common to apply approximate solvers for
  the inverse, for instance using low-rank approximation based on
  \textsc{SVD}-decomposition, which we will briefly turn to in the following.

  \subsection{Singular value decomposition.}
  
  Recall that any matrix \( \mat{A} \in \C^{n, m} \) can be decomposed as
  \begin{equation}
    \mat{A} = \mat{U} \mat{\Sigma}\mat{V}^T,
  \end{equation}
  where \( \mat{U} \) and \( \mat{V} \) are comprised of the eigenvectors of \(
  \mat{A}\mat{A}^T \) and \( \mat{A}^T\mat{A} \) respectively. As these
  eigenvectors are orthonormal, it follows that both \( \mat{U} \) and \(
  \mat{V} \) are unitary matrices. Furthermore, \( \mat{\Sigma} \) is a matrix 
  \begin{equation}
    \mat{\Sigma} = \begin{bmatrix}
      \Sigma_1 & 0 \\
       0 & 0
    \end{bmatrix},
  \end{equation}
  where \( \Sigma_1 \) is a square diagonal matrix of size \( r \times r \)
  with the non-zero singular values of \( \mat{A} \). The integer \( r \) is
  the \emph{rank} of \( \mat{A} \). As the matrix \( \Sigma \) is mostly
  containing zeros, the information stored in \( \mat{A} \) is attributed to
  only some parts of \( \mat{U} \) and \( \mat{V} \). We can remove the
  redundant parts, and more compactly express \( \mat{A} \) as \( \mat{A} =
  \mat{U}_1 \mat{\Sigma}_1 \mat{V}_1^T \) without loss of accuracy in the
  decomposition. Here, \( \mat{U}_1 \) is \( m \times r \) and \( \mat{V}_1 \)
  is \( n \times r \).

  Applying the singular value decomposition to the matrix \(\X = \mat{U}
  \mat{\Sigma} \mat{V}^T\) we can analyze the expression for the prediction \( \hat{\y} \)
  in terms of the matrix \( \X^T \X \).  First of all, we have that
  \begin{align}
    \begin{split}
    \X^T \X &= \mat{V}\mat{\Sigma}^T\mat{U}^T\mat{U}\mat{\Sigma} \mat{V}^T\\
            &= \mat{V}\mat{D}\mat{V}^T, 
    \end{split}
  \end{align}
  where we have used that \( \mat{U}\mat{U}^T = \mat{I} \) and defined \(
  \mat{D} = \diag{\sigma_1^2, \ldots, \sigma_n^2} \).  Thus, plugging this into
  \cref{eq:optimal,eq:matrix_prod}, we obtain the expression
  \begin{align}
    \begin{split}
      \hat{\y} = \X \beta = \X (\mat{V}\mat{D}\mat{V}^T)^{-1} \mat{X}^T \y.
    \end{split}
  \end{align}
  Finally, substituting \( \X = \mat{U}\mat{\Sigma}\mat{V}^T \) and noting that
  diagonal matrices always commute, we end up with
  \begin{align}
    \begin{split}
      \hat{\y} = \mat{U}\mat{U}^T \mat{y}.
    \end{split}
  \end{align}

  \subsection{The bias-variance tradeoff} 
  
  Since we are attempting to estimate a true distribution based on finite
  samples, there are certain statistical properties it is worthwile discussing.
  We follow the derivation in.  Recall \cref{as:general_linear} which states
  that our true data is generated from a noisy model
  \begin{equation}
    y = f(\x) + \varepsilon
  \end{equation}
  with \( \varepsilon \sim \N(0, \sigma^2) \).  We select a finite sample from
  the true data, which constitutes our finite data-set \( \data \).
  Furthermore, we have a model function \( f(\data, \beta) \) which depends on
  the selected data-set and the model parameters \( \beta \). Thus, our
  cost-function \( \cost \) will depend on the specific sample taken from the
  true distribution. It is of interest to examine the expected out-of-sample
  error \( \expect_{\data, \varepsilon}[\cost(\data, \beta)] \) of the model
  function over the sampled data set \( \data \) and the noise \( \varepsilon
  \). This can be decomposed into three distinct parts,
  \emph{\textcolor{Maroon}{variance}}, \emph{\textcolor{Blue}{bias}}, and
  \emph{\textcolor{Green}{noise}}:
  \begin{equation}
    \expect_{\data, \varepsilon}[\cost(\data, \beta)] = \frac{1}{n} \textcolor{Blue}{\sum_{i=1}^n \left(f(\x_i) - \expect_{\data}\left[\hat{y}_i\right]\right)^2}  + \frac{1}{n}\textcolor{Maroon}{\sum_{i=1}^n \expect_{\data}\big[\left(\hat{y}_i - \expect_{\data} [\hat{y}_i]\right)^2\big]} + \textcolor{Green} \sigma^2.
  \end{equation}
  See \cref{ap:derivation} for the full derivation. The bias-term measures the
  deviation of the expectation of the model function \(f(\data, \beta)\) from
  the true value \( f \), while the variance-term measures the fluctuations
  arising due to using finite samples.

  \section{Ridge Regression}
  
  When the dimensionality of the matrix \( \X \) is high, the problem of
  near-collinear columns often arise, as mentioned earlier. Thus, the model
  parameters \( \beta_{\OLS} \) of the ordinary least squares regression cannot
  be determined. The propsed fix to this problem is to slightly perturb the
  matrix \( \X^T \X \) by adding small positive values along the diagonal to
  ensure its invertability. This is known as \emph{ridge regression} and yields
  a model parameter estimator
  \begin{equation}
    \beta_{\ridge}(\lambda) = (\X^T\X + \lambda\mat{I})^{-1}\X^T\y
  \end{equation}
  where \( \lambda \geq 0 \) is a hyper parameter. To see why the resulting
  modified matrix is invertible, one has to look at the singular values.  It
  turns out that the model parameters \( \beta_{\ridge}(\lambda) \) directly
  minimize the \emph{ridge loss function} defined as
  \begin{equation}
    \cost_{\ridge}(\data, \beta;  \lambda) \coloneqq \frac{1}{n} \sum_{i=1}^n (y_i - \hat{y}_i)^2 + \lambda \sum_{i=1}^p \beta_i^2 = \cost(\data, \beta) + \lambda \beta^T \beta,
  \end{equation}
  which we recognize as standard penalized least squares.
  
  Ridge regregression carries several benefits. One of the more important is
  its resilience to overfitting. In a high-dimensional setting, if many of the
  data-features can be attributed to a few covariates, then the remaining
  covariates do nothing more than fit to the noise. Large values of \( \beta_i
  \) then a good indication that your model might be overfitting as some
  covariates are given more weight than others.  The Ridge regression solves
  this by directly penalizing large model parameters in the cost function.
  
  \appendix
  \chapter{Derivation of the bias-variance-noise relationship}
  \label{ap:derivation}
  
  \if 0
  To this end --- by adding and subtracting \( f(\x_i) \) --- we have that 
  \begin{align}
    \begin{split}
      \expect_{\data, \varepsilon} \big[\cost(\data, \beta)\big] &= \frac{1}{n}\expect_{\data, \varepsilon} \left[\sum_{i = 1}^n (y_i - \hat{y}_i)^2\right] \\
                                                                 &= \frac{1}{n}\expect_{\data, \varepsilon} \left[\sum_{i=1}^n (y_i - f(\x_i) + f(\x_i) - \hat{y}_i)^2\right] \\
                                                                 &= \frac{1}{n}\sum_{i=1}^n\left\{\overbrace{\expect_{\varepsilon} \left[(y_i - f(\x_i))^2\right]}^{= \sigma^2} + \expect_{\data}\left[(f(\x_i) - \hat{y}_i)^2\right] + 2\overbrace{\expect_{\varepsilon}\left[y_i - f(\x_i)\right]}^{= 0}\expect_{\data}\left[f(\x_i) - \hat{y}_i\right] \right\} \\ 
                                                                 &= \frac{1}{n}\left\{n\sigma^2 + \expect_{\data} \left[(f(\x_i) - \hat{y}_i)^2\right]\right\} \\
                                                                 &= \sigma^2 + \frac{1}{n}\sum_{i=1}^n\expect_{\data} \left[(f(\x_i) - \hat{y}_i)^2\right]\right\}.
  \end{split}
  \end{align}

  Using the same trick of adding and subtracting, we may further decompose the second term as
  \begin{align}
    \begin{split}
      \expect_{\data} \big[ (f(\x_i) - \hat{y}_i)^2\big] = \left(f(\x_i) - \expect_{\data}\big[\hat{y_i}\big]\right)^2 + \expect_{\data} \left[\left(\hat{y}_i - \expect_{\data}\left[\hat{y}_i\right]\right)^2\right]
    \end{split}
  \end{align}
  \fi
  Combining the two previous equations, we obtain
  
\end{document} 

\documentclass[article, a4paper, oneside]{memoir}
\linespread{1.25}

\setlength{\parskip}{1em}
\setlength{\parindent}{0em}
\usepackage[]{eulervm} 
\usepackage[]{palatino} 
\usepackage[]{graphicx} 
\usepackage[]{commath} 
\usepackage[]{booktabs} 
\usepackage[]{microtype} 
\usepackage[]{geometry} 
\usepackage[]{minted} 
\usemintedstyle{algol}

\usepackage[]{amsmath, amssymb, amsthm, mathtools} 
\usepackage[backend=biber, bibencoding=utf8, style=ieee]{biblatex} 
\addbibresource{../../FYS-STK4155.bib}

\usepackage[]{hyperref} 
\usepackage[noabbrev, capitalize]{cleveref} 

\newcommand{\cost}{\mathcal{C}}

\newcommand*{\github}[1]{\def\@github{\color{gray!70}#1}}
\newcommand{\githublogo}{\raisebox{-1pt}{\includegraphics[height=9pt]{imgs/github}}}
\title{\textsc{Solving the heat-equation using neural networks}}
\author{Ivar Stangeby}

\begin{document}
	\maketitle	

	\begin{abstract}
		In this project we compare a standard finite difference method
		to neural networks in the solution of the heat equation. We
		show that the neural network severely underperforms when
		compared to the finite differences, and that the run-time is
		significantly slower. This gives us no immediate reason to
		chose a network based approach over classical methods. All
		relevant code can be found on GitHub:

		\centering{\href{https://github.com/qTipTip/FYS-STK4155}{\githublogo \, \color{gray!50}
github.com/qTipTip }}

	\end{abstract}

	\chapter{Introduction}

	Partial differential equations have widespread use in the sciences, in
	fields ranging from structural engineering to economics. The solution
	of such equations in an accurate and precise way is of paramount
	importance.

	Several approaches to the solution of such equations exists, with the
	more notable being \emph{finite difference} and \emph{finite
	element}-methods. These two methods approach the problem at hand from
	two different angles. In the finite difference schemes, the
	differential operator is discretized, typically using schemes arising
	from Taylor approximations to function. Finite element methods on the
	other hand, instead of discretizing the operators, discretize the
	function space for which the solution is sought.

	In recent years, the use of artificial intelligence in the solution of
	partial differential equations has started to gain some traction, see
	for instance the DeepXDE-network \cite{luDeepXDEDeepLearning2019} and
	the Deep Galerkin Method (DGM)
	\cite{al-aradiSolvingNonlinearHighDimensional}.
	
	The neural network approach to solving PDEs is meshless, that is, it
	does not rely on any underlying discretization of the domain as opposed
	to finite difference and finite element methods. Furthermore, the
	boundary conditions of the problem are encoded directly in the
	cost-function used to optimize the neural network. In finite elements
	the solution space is constructed in such a way as to automatically
	satisfy the boundary conditions, and in finite differences the boundary
	conditions must be imposed explicitly.

	\chapter{Theory}

	\section{The heat equation}	

	We wish to solve the one-dimensional heat-equation. The problem reads
	as follows. Find \( u \) such that
	\begin{equation}
		\label{eq:heat_eq}
		\pd[2]{u(x, t)}{x} = \pd{u(x, t)}{t}
	\end{equation}
	subject to the boundary conditions \( u(0, t) = u(1, t) = 0 \) and the
	initial condition
	\begin{equation}
		u(x, 0) = \sin(\pi x).
	\end{equation}
	We interpret the problem as that of finding the temperature gradient in
	a rod of fixed length \( L = 1\).

	\subsection{An analytical solution}
	
	In order to assess a-posteriori model performance, we need to know the
	exact solution to our specific instance of the heat equation.

	Following \cite{hancockLinearPartialDifferential} we derive an
	analytical solution.  We assume separation of variables and see whether
	this leads us to anything conclusive. We write
	\begin{equation}
		u(x, t) = X(x)T(t)
	\end{equation}
	where \( X \) carries the spatial dependence and \( T \) carries the
	temporal dependence of the problem.  By taking the required partial
	derivatives we obtain \(X''(x)T(t) = X(x)T'(t)\) from which we can
	deduce that the ratios
	\begin{equation}
		\label{eq:ratio}
		\frac{X''(x)}{X(x)} = \frac{T'(t)}{T(t)} = -\lambda
	\end{equation}
	are constant. From the boundary conditions, we have that
	\begin{align}
		u(0, t) = X(0)T(t) = 0, \\
		u(1, t) = X(1)T(t) = 0.
	\end{align}
	By examination, we note that if \(T(t) = 0\) we obtain the trivial
	solution \( u = 0\), which does not satisfy the boundary conditions.
	Hence \( T \neq 0 \). Thus to satisfy the boundary conditions we must
	have \( X(0) = X(1) = 0 \).
	
	The ratios in \cref{eq:ratio} gives rise to two ODEs, one in time and
	one in space:
	\begin{align}
		X''(x) + \lambda X(x) &= 0, \label{eq:spatialODE} \\
		T'(t) + \lambda T(t) &= 0 \label{eq:temp_ODE}.
	\end{align}
	We have obtained boundary conditions for the spatial ODE, hence we
	start by solving that.
	
	As the constant \( \lambda \) is unknown, we have to consider three
	cases. The cases \( \lambda < 0 \) and \( \lambda = 0 \) can be shown
	to lead to \( u = 0\) which we discard. Thus we assume \( \lambda >
	0\).  The solution to \cref{eq:spatialODE} is then
	\begin{equation}
		X(x) = A\cos(\sqrt{\lambda}x) + B\sin(\sqrt{\lambda}x).
	\end{equation}
	By imposing the boundary conditions we obtain \( A = 0 \) and \(
	B\sin(\sqrt{\lambda}) = 0 \). Again, if we allow \( B = 0\), we obtain
	\( u = 0\), so we discard this possibility. Hence \( \sin(\sqrt{\lambda}) = 0
	\) from which we can deduce that
	\begin{equation}
		\lambda = \pi^2 n^2 
	\end{equation}
	for \( n \in \mathbb{N}^+\). This tells us that we have an infinite
	number of solutions, one for each \( n \):
	\begin{equation}
		\label{eq:solution_space}
		X_n(x) = B_n\sin(\pi n x)
	\end{equation}
	for some unknown constant \( B_n \).
	

	We now solve for the spatial component \( T \). For each possible
	choice of \( \lambda \) we obtain
	\begin{equation}
		T_n'(t) +\pi^2 n^2T_n(t) = 0, 
	\end{equation}
	which has solutions
	\begin{equation}
		\label{eq:solution_temp}
		T_n(t) = C_n e^{-n^2\pi^2t}.
	\end{equation}
	Combining \cref{eq:solution_space,eq:solution_temp} we get the family of solutions
	\begin{equation}
		u_n(x, t) = B_n C_n \sin(n\pi x)e^{-n^2\pi^2t}.
	\end{equation}
	In principle, we would have to consider the fourier coefficients of the
	infinite sum of these solutions, however, by examining our initial
	conditions, we see that \( u_1 \) is the desired solution, with \( B_1
	C_1 = 1 \).

	Thus, our closed form solution to the one-dimensional heat equation
	with our prescribed boundary and initial conditions is:
	\begin{equation}
		u(x, t) = \sin(\pi x) e^{-\pi^2 t}.
	\end{equation}
	
	\section{Finite differences}

	We wish to solve the heat equation over the domain \( \Omega = [0, L]
	\times [0, T]  \). Discretizing using \( N \) grid points in the
	spatial direction and \( M \) grid points in the temporal direction, we
	obtain step sizes \( \Delta x = L / (N - 1) \) and \( \Delta t = T / (M
	- 1) \). Of the myriad of different finite difference schemes, we
	choose the \emph{forward in time---centered in space} (FTCS).
	
	\subsection{Forward in time}
	We start by discretizing the time-differential in \cref{eq:heat_eq} by
	taking a first-order Taylor approximation of \( u \). We have
	\begin{equation}
		u(x, t + \Delta t) = \sum_{n = 0}^\infty \frac{\Delta t^n}{n!} \pd[n]{u(x, t)}{t} = u(x, t) + \Delta t \pd{u(x, t)}{t} + \mathcal{O}(\Delta t).
	\end{equation}
	We discard higher order terms and rearrange, yielding
	\begin{equation}
		\label{eq:forward_time}
		\pd{u(x, t)}{t} \approx \frac{u(x, t+\Delta t) - u(x, t)}{\Delta t}, 
	\end{equation}
	with a truncation error that goes as \( \mathcal{O}(\Delta t) \).
	
	\subsection{Centered in space}	

	The centered difference scheme involves two second-order
	Taylor-aproximations in space:
	\begin{align}
		u(x + \Delta x, t) &= u(x, t) + \Delta x \dpd{u(x, t)}{x} +
		\frac{\Delta x^2}{2} \dpd[2]{u(x, t)}{x} + \mathcal{O}(\Delta x^2) \\
		u(x - \Delta x, t) &= u(x, t) - \Delta x \dpd{u(x, t)}{x} + \frac{\Delta x^2}{2} \dpd[2]{u(x, t)}{x} + \mathcal{O}(\Delta x^2).
	\end{align}
	Truncating, adding the equations to elimiate the first order
	derivatives, and solving for the second derivative yields:
	\begin{equation}
		\label{eq:centered_space}
		\dpd[2]{u(x, t)}{x} \approx \frac{u(x + \Delta x, t) - 2 u(x,
		t) + u(x - \Delta x, t)}{\Delta x^2}
	\end{equation}
	with a truncation order of \( \mathcal{O}(\Delta x^2) \).

	\subsection{The full scheme}	

	Plugging \cref{eq:forward_time,eq:centered_space} into the heat
	equation given in \cref{eq:heat_eq} yields
	\begin{equation}
		\frac{u(x, t + \Delta t) - u(x, t)}{\Delta t} = \frac{u(x +
		\Delta x, t) - 2u(x, t) + u(x - \Delta x, t)}{\Delta x^2}.
	\end{equation}
	We are interesting in solving the equation forwards in time, so we
	solve for the term \( u(x, t + \Delta t) \), yielding:
	\begin{equation}
		u(x, t + \Delta t) = \left( u(x + \Delta x, t) + u(x - \Delta x, t) \right) \frac{\Delta t}{\Delta x^2} + \left(1 - 2 \frac{\Delta t}{\Delta x^2}\right)u(x, t).
	\end{equation}
	Note that the solution at the next time step requires three spatial
	solutions at the previous time-step. Hence, we are reliant on the
	boundary conditions to keep the algorithm running.

	While this scheme is easy to implement, it suffers from stability
	issues, which can be combated by tweaking the ratio \( \Delta t /
	\Delta x^2\). By ensuring that 
	\begin{equation}
		\Delta t \leq \frac{\Delta x^2}{2}, 
	\end{equation}
	the method behaves nicely.

	\section{Neural network in the context of PDEs}
	
	Consider a neural network \( f:\Omega \to \mathbb{R}\) parameterized by \(\theta\).  The
	question is now, how do we embed the partial differential equation in
	the neural network? The answer lies in how we engineer our loss
	function. 
	
	\subsection{Criterion}	

	From \cref{eq:heat_eq} we can see that we wish to minimize
	the following expression:
	\begin{equation}
		\label{eq:min_residual}
		\min_{\theta} \left[\dpd[2]{f(x, t; \theta)}{x} - \dpd{f(x, t; \theta)}{t}\right]
	\end{equation}
	for \( x \in [0, L] \) and \( t > 0\). Thus, we can simply use the mean
	squared error of the residuals in the right hand side of
	\cref{eq:min_residual}. We therefore define our cost function \( \cost
	\) as the mean squared residual error over all (assume \( n \))
	data-points  in \(\Omega = [0, L] \times [0, T]\).
	\begin{equation}
		\cost(f) = \frac{1}{n} \sum_{i=1}^n \left(  \dpd[2]{f(x_i, t_i)}{x}
		- \dpd{f(x_i, t_i)}{t}\right)^2
	\end{equation}
	
	\subsection{Embedding boundary conditions}

	Currently, we are not imposing any boundary conditions. Thus, by
	naively running the network as is, we may find any function whose
	second partial derivative in space is equal to its partial derivative
	in time. There are several approaches to solving this problem. In
	\cite{luDeepXDEDeepLearning2019} they train the network according to a
	multi-task loss following the same trick as in \cref{eq:min_residual}
	but for data-points lying on the boundary.
	
	We instead opt for the method of constructing a \emph{trial function}
	that acts as a mediator between the neural network and the
	loss-function which ensures that the network learns to satisfy the
	boundary conditions.

	Recall from our boundary and initial conditions that we require \( u(0,
	t) = u(1, t) = 0 \) and \( u(x, 0) = \sin(\pi x) \). The function \( g
	\) defined by
	\begin{equation}
		\label{eq:trial}
		g(x, t; \theta) = (1 - t) \sin(\pi x) + x(1-x)t f(x, t; \theta),
	\end{equation}
	satisfies these conditions. It is therefore this function we pass into
	our cost function \( \cost \).
	
	\subsection{Neural network architecture}

	As our architecture we wanted to reuse the \textsc{GenericNN}
	implemented in \cite{stangebyFYSSTK4155Project2019a}, which is a
	feed-forward neural network with \textsc{Relu}-activations at each
	hidden layer. However, after initial testing, using \textsc{Relu}
	yielding wildly oscilating loss, hence we chose the more stable
	\textsc{Sigmoid}-activation. This led to slower learning rates, due to
	the saturation of gradients, but allowed the network to get a tighter
	approximation.
	
	\section{Performance metrics}
	
	In order to evaluate our numerical approximations, whether they are
	computed using finite-difference or they are trained neural networks,
	we have several metrics we can consider. Calling our numerical
	approximation to \( u \) for \( \tilde{u} \), we define our errors as
	\begin{equation}
		e_p = \| u - \tilde{u} \|_p,
	\end{equation}
	where \( \| \cdot \|_p \) denotes the matrix \(p\)-norm. We consider in
	particular the \( \infty\)-norm, coinciding with the maximum absolute
	error, and the \( 2 \)-norm, which is the standard euclidean
	(Frobenius) norm.

	\chapter{Implementation}
	
	The finite difference solver is implemented using a naive for-loop
	approach in \texttt{numpy} and can be seen in \cref{lst:ftcs}. In order
	to satisfy the stability criterion, we choose our desired \( \Delta x
	\) and compute a corresponding \( \Delta t\) by
	\begin{equation}
		\Delta t = \Delta x^2 / 2.
	\end{equation}

	The neural network is implemented in \texttt{tensorflow}, which takes
	care of the backpropagation of gradients through the network using the
	\texttt{AutoGrad}-package. Ideally, we wanted to implement this in
	\texttt{PyTorch}, however, taking element-wise gradients of the network
	output with the respect to the network input, as is required to
	evaluate \cref{eq:trial}, turned out to be hard to do in
	\texttt{PyTorch}. Due to my lacking experience in \texttt{Tensorflow},
	the implementation is a modified version of the one used to solve the
	wave equation in \cite{heinDataAnalysisMachine}. This was implemented
	in \texttt{Tensorflow V1}, thus we had to add the compatibility
	modifications shown in \cref{lst:compatibility} in order for this to
	run using the newer \texttt{Tensorflow V2} API.
	
	\begin{listing}
		\centering	
		\begin{minted}{python}
import tensorflow.compat.v1 as tf
tf.disable_eager_execution()		
		\end{minted}
		\caption{Compatibility modification needed to use the
		deprecated functionality of \texttt{Tensorflow} V1 in the V2
	API.}
		\label{lst:compatibility}
	\end{listing}

	
	\begin{listing}
		\centering
		\begin{minted}{python}
def ftcs(space_resolution, time_resolution, space_min_max=[0, 1],
		time_max=1, boundary_conditions=[0, 0], initial_condition=None):
    """
    Solves the 1D-heat equation using a forward-in-time 
    centered-in-space discretization scheme.

    :param space_resolution: number of points in spatial direction
    :param time_resolution: number of points in temporal direction
    :param space_min_max: the spatial boundary values
    :param time_max: the time to run the simulation for
    :param boundary_conditions: the boundary conditions at the space_min_max-values
    :param initial_conditions: the initial conditions at time = 0, callable.
    """

    if initial_condition is None:
        initial_condition: lambda x: 0

    dt = time_max / time_resolution
    u0, dx = np.linspace(*space_min_max, num=space_resolution, retstep=True)
    u = np.zeros((time_resolution, space_resolution))
    u[:, [0, -1]] = boundary_conditions
    u[0] = initial_condition(u0)

    F = dt / dx ** 2
    for step in tqdm.trange(time_resolution - 1):
        for i in range(1, space_resolution - 1):
            u[step + 1, i] = u[step, i] +\ 
	    	F * (u[step, i - 1] - 2 * u[step, i] + u[step, i + 1])

    return u			

@np.vectorize
def initial_condition(x):
    return np.sin(np.pi * x)
		\end{minted}
		\caption{A \texttt{numpy}-based implementation of the
			\emph{forward in time---centered in space} scheme. The
			initial conditions are passed in as a callable, and the
			function returns the solution at all time-steps in form
			of a \texttt{time\_resolution} \(\times\)
		\texttt{space\_resolution} matrix.}
		\label{lst:ftcs}
	\end{listing}

	\chapter{Numerical experiments}
	
	We start by running the forward in time--central in space scheme. The
	errors are reported in \cref{tab:ftsc_errors}. The 2-norm steadily
	decreases, which means that on average the approximation becomes better
	with decreased grid-size. Also, the \( \infty\)-norm decreases steadily
	towards zero. With this in mind, the approximations are fairly good at
	even low spatial resolutions. Due to the stability criterion, running
	this method for even smaller grid-sizes in space quickly becomes
	infeasible.

	\begin{table}[htpb]
		\centering
		\caption{Solving the heat equation for varying spatial
			resolution, with the temporal resolution chosen to
			satisfy the stability criterion. We report both the \(
			e_1 \) and \( e_2 \) errors.} 
		\label{tab:ftsc_errors}
		\begin{tabular}{lrr}
			\toprule
			\textsc{FTCS} & \( e_\infty \) & \( e_2 \) \\
			\midrule
			\( \Delta x = 1 / 10 \) & \num{0.00357} & \num{0.04674} \\
			\( \Delta x = 1 / 20 \) & \num{0.00098} & \num{0.03688} \\
			\( \Delta x = 1 / 30 \) & \num{0.00044} & \num{0.03113} \\
			\( \Delta x = 1 / 40 \) & \num{0.00025}& \num{0.02743} \\
			\( \Delta x = 1 / 100 \) & \num{0.00004} &\num{0.01787} \\
			\bottomrule
		\end{tabular}
	\end{table}
	
	We now run the neural network. During testing we realized that the
	neural network also needs a balance between the number of data-points
	in the temporal domain and the number of data-points in the spatial
	domain. We therefore run the network with both spatial and temporal
	resolution increasing at the same rate, following that of
	\cref{tab:ftsc_errors}. We evaluate the same errors. The network is run
	for \( 10000 \) epochs (which in hindsight might be on the fewer end of
	things), and the results are shown in \cref{tab:nnpde_errors}. Note in
	particular how the error not neccesarily decreases with higher
	resolution. This might indicate that a different architecture is to be
	prefered. Maybe a shallower network with larger width. This will also
	reduce the run-time of the network, as the current architecture is
	quite sluggish.
	
	Taking the reported errors at face-value, the classical finite
	difference method is to prefer, despite its stability issues.


	\begin{table}[htpb]
		\centering
		\caption{Solving the heat equation for identical spatial and
			teporal resolution.  In addition to the errors not
			neccesarily decreasing on increased resolution, the
			network complexity quickly increases, which incurs
			significant run-time increase. This might indicate that
			a different architecture is to be prefered.
			} 
		\label{tab:nnpde_errors}
		\begin{tabular}{lrr}
			\toprule
			\textsc{FTCS} & \( e_\infty \) & \( e_2 \) \\
			\midrule
			\(\Delta t = \Delta x = 1 / 10 \) & \num{0.03684} & \num{0.14905} \\
			\(\Delta t =  \Delta x = 1 / 20 \) & \num{0.14728} & \num{1.24897} \\
			\(\Delta t =  \Delta x = 1 / 30 \) & \num{0.03979} & \num{0.39177} \\
			\(\Delta t =  \Delta x = 1 / 40 \) & \num{0.04963}& \num{0.55036} \\
			\(\Delta t =  \Delta x = 1 / 100 \) & \num{0.05103} &\num{1.35165} \\
			\bottomrule
		\end{tabular}
	\end{table}

	\chapter{Conclusion}
	
	In this project we have discussed the solution of the heat-equation,
	both using finite-difference methods, and a neural network based
	approach. Based on the results, the \emph{forward in time---centered in
	space} difference scheme outperformed the neural net both in accuracy
	and in runtime, despite its stability issues. 

	The benefits of using a neural network for this problem is hard to see
	at face value. One benefit is however that the neural network does not
	need an underlying domain discretization. This does not constitute any
	advantage in such a simple case as our 1D diffusion equation, but in
	higher dimensions, this can prove to be a great advantage.

	For instance in the finite element method, the numerical properties of
	the approximation greatly hinges on the quality of the underlying mesh. 

	The poor results of the neural network sparked an interest in trying
	out different architectures, however I spent too much time trying to
	make this work using \texttt{PyTorch}, and realized too late that I had
	to migrate to \texttt{Tensorflow}, where my experience is lacking.
	
	In conclusion, for simple tasks like this, it seems like the neural
	network works well, comparatively, for very coarse domain
	discretizations, however falls behind finer resolutions.

	Outside of the scope of this project would be the solution of the 2D
	wave equation using a neural network over a non-trivial domain, which
	might be able to highlight some of the benefits of not requiring domain
	discretization.
\clearpage

\appendix

\chapter{Finding eigenvalues}

In this appendix we briefly discuss the idea behind how you can estimate
eigenvalues and eigevectors of a real symmetric matrix using the neural network
framework discussed in this project. Unfortunately, I was unable to make the
network converge, and due to time-constraints this was relegated to an
appendix.

Following \cite{yiNeuralNetworksBased2004} we can also use our neural network
to find the eigenvalues of a real symmetric matrix \( A \). In particular, let
\( Q \)  be a \( 6 \times 6 \) random matrix and define
\begin{equation}
	A = \frac{Q^T + Q}{2}, 
\end{equation}
which then is a symmetric real matrix.

In \cite{yiNeuralNetworksBased2004} they show that any non-zero equilibrium
point of the system
\begin{equation}
	\pd{u(t)}{t} = -u(t) + f(u(t)), 
\end{equation}
where \( f(u) \) is defined as
\begin{equation}
	f(u) = \left[u^Tu A + \left(1 - u^TAu\right)I\right]u
\end{equation}
is an eigenvalue of the matrix \( A \).
Here \( u \) is the network output.

To recast this into our framework, we wish to minimize the function 
\begin{equation}
	\pd{u(t)}{t} + u(t) - f(u(t))
\end{equation}
with respect to our network parameters. The network is shown to converge to
\emph{an} eigenvalue of the matrix \( A \). However, you can make the network
converge to the largest eigenvalue by choosing an initial condition for the
network non-orthogonal to the eigenspace of the largest eigenvalue. This
eigenspace is not known a-priori, so a random initialization is made with the
motivation that the probability is rather large that the random vector is not
orthogonal to the eigenspace.
\printbibliography
\end{document}
